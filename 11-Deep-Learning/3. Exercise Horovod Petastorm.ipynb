{"cells":[{"cell_type":"markdown","source":["## Exercise: Horovod with Petastorm for training a deep learning model\n\nIn this exercise we are going to build a model on the Boston housing dataset and distribute the deep learning training process using both HorovodRunner and Petastorm.\n\n**Required Libraries**: \n* `petastorm==0.8.2` via PyPI"],"metadata":{}},{"cell_type":"markdown","source":["Run the following cell to set up our environment."],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["## 1. Load and process data\n\nWe again load the Boston housing data. However, as we saw in the demo, for Horovod we want to shard the data before passing into HorovodRunner. \n\nFor the `get_dataset` function below, load the data, split into 80/20 train-test, standardize the features and return train and test sets."],"metadata":{}},{"cell_type":"code","source":["from sklearn.datasets import load_boston\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\ndef get_dataset(rank=0, size=1):\n  scaler = StandardScaler()\n  \n  boston_housing = load_boston()\n\n  # split 80/20 train-test\n  X_train, X_test, y_train, y_test = train_test_split(boston_housing.data,\n                                                          boston_housing.target,\n                                                          test_size=0.2,\n                                                          random_state=1)\n  \n  scaler.fit(X_train)\n  X_train = scaler.transform(X_train[rank::size])\n  y_train = y_train[rank::size]\n  X_test = scaler.transform(X_test[rank::size])\n  y_test = y_test[rank::size]\n  \n  return (X_train, y_train), (X_test, y_test)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["##2. Build Model\n\nUsing the same model from earlier, let's define our model architecture"],"metadata":{}},{"cell_type":"code","source":["import numpy as np\nnp.random.seed(0)\nimport tensorflow as tf\ntf.set_random_seed(42) # For reproducibility\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\ndef build_model():\n  return Sequential([Dense(50, input_dim=13, activation='relu'),\n                    Dense(20, activation='relu'),\n                    Dense(1, activation='linear')])"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["## 3. Horovod\n\nIn order to distribute the training of our Keras model with Horovod, we must define our `run_training_horovod` training function"],"metadata":{}},{"cell_type":"code","source":["# TODO\nimport horovod.tensorflow.keras as hvd\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.callbacks import *\n\ndef run_training_horovod():\n  # Horovod: initialize Horovod.\n  hvd.init()\n  print(f\"Rank is: {hvd.rank()}\")\n  print(f\"Size is: {hvd.size()}\")\n  \n  FILL_IN # LOAD DATA\n  \n  model = FILL_IN\n  from tensorflow.keras import optimizers\n  optimizer = FILL_IN\n  optimizer = FILL_IN\n  \n  model.compile(optimizer=optimizer, loss=\"mse\", metrics=[\"mse\"])\n  checkpoint_dir = f\"{ml_working_path}/horovod_checkpoint_weights_lab.ckpt\"\n  \n  callbacks = FILL_IN\n  \n  # Horovod: save checkpoints only on worker 0 to prevent other workers from corrupting them.\n  if hvd.rank() == 0:\n    callbacks.append(ModelCheckpoint(checkpoint_dir, save_weights_only=True))\n  \n  # (make sure you use batch_size of 16 for the learning rate warmup callback, or else you might get a division by 0 error with this small dataset)\n  history = model.fit(X_train, y_train, batch_size=16, FILL_IN)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["Let's now run our model on all workers."],"metadata":{}},{"cell_type":"code","source":["# TODO\nfrom sparkdl import HorovodRunner\n\nhr = FILL_IN"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["## 4. Horovod with Petastorm\n\nWe're now going to build a distributed deep learning model capable of handling data in Apache Parquet format. To do so, we can use Horovod along with Petastorm. \n\nFirst let's load the Boston housing data, and create a Spark DataFrame from the training data."],"metadata":{}},{"cell_type":"code","source":["import pandas as pd\n\nboston_housing = load_boston()\n\n# split 80/20 train-test\nX_train, X_test, y_train, y_test = train_test_split(boston_housing.data,\n                                                        boston_housing.target,\n                                                        test_size=0.2,\n                                                        random_state=1)\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# concatenate our features and label, then create a Spark DataFrame from our Pandas DataFrame.\ndata = pd.concat([pd.DataFrame(X_train, columns=boston_housing.feature_names), \n                  pd.DataFrame(y_train, columns=[\"label\"])], axis=1)\ntrainDF = spark.createDataFrame(data)\ndisplay(trainDF)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["### Create Vectors\n\nUse the VectorAssembler to combine all the features (not including the label) into a single column called `features`."],"metadata":{}},{"cell_type":"code","source":["# TODO\nfrom pyspark.ml.feature import VectorAssembler\n\nvecAssembler = FILL_IN\nvecTrainDF = FILL_IN"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["Let's now create a UDF to convert our Vector into an Array."],"metadata":{}},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.linalg.Vector\nval toArray = udf { v: Vector => v.toArray }\nspark.udf.register(\"toArray\", toArray)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["Save the DataFrame out as a parquet file to DBFS. \n\nLet's remember to remove the committed and started metadata files in the Parquet folder! Horovod with Petastorm will not work otherwise."],"metadata":{}},{"cell_type":"code","source":["file_path = f\"{workingDir}/petastorm.parquet\"\nvecTrainDF.selectExpr(\"toArray(features) AS features\", \"label\").repartition(8).write.mode(\"overwrite\").parquet(file_path)\n[dbutils.fs.rm(i.path) for i in dbutils.fs.ls(file_path) if (\"_committed_\" in i.name) | (\"_started_\" in i.name)]"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["Let's now define our `run_training_horovod` to format our data using Petastorm and distribute the training of our Keras model using Horovod."],"metadata":{}},{"cell_type":"code","source":["# TODO\nfrom petastorm import make_batch_reader\nfrom petastorm.tf_utils import make_petastorm_dataset\nimport horovod.tensorflow.keras as hvd\n\nabs_file_path = file_path.replace(\"dbfs:/\", \"/dbfs/\")\n\ndef run_training_horovod():\n  # Horovod: initialize Horovod.\n  hvd.init()\n  with make_batch_reader(\"file://\" + abs_file_path, \n                         num_epochs=100, \n                         cur_shard=hvd.rank(), \n                         shard_count= hvd.size()) as reader:\n    \n    dataset = FILL_IN\n    model = FILL_IN\n    from tensorflow.keras import optimizers\n    optimizer = FILL_IN\n    optimizer = FILL_IN\n    \n    model.compile(optimizer=optimizer, loss='mse')\n    \n    checkpoint_dir = f\"{ml_working_path}/petastorm_checkpoint_weights_lab.ckpt\"\n    \n    callbacks = [\n      hvd.callbacks.BroadcastGlobalVariablesCallback(0),\n      hvd.callbacks.MetricAverageCallback(),\n      hvd.callbacks.LearningRateWarmupCallback(warmup_epochs=5, verbose=1),\n      ReduceLROnPlateau(monitor=\"loss\", patience=10, verbose=1)\n    ]\n\n    # Horovod: save checkpoints only on worker 0 to prevent other workers from corrupting them.\n    if hvd.rank() == 0:\n      callbacks.append(ModelCheckpoint(checkpoint_dir, save_weights_only=True))\n\n    history = FILL_IN # (use steps_per_epoch=10)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["Finally, let's run our newly define Horovod training function with Petastorm to run across all workers."],"metadata":{}},{"cell_type":"code","source":["# TODO\nfrom sparkdl import HorovodRunner\n\nhr = FILL_IN"],"metadata":{},"outputs":[],"execution_count":23}],"metadata":{"name":"3. Exercise Horovod Petastorm","notebookId":3701447417106916},"nbformat":4,"nbformat_minor":0}