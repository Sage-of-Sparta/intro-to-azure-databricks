{"cells":[{"cell_type":"markdown","source":["# Horovod with Petastorm\n\n[Petastorm](https://github.com/uber/petastorm) enables single machine or distributed training and evaluation of deep learning models from datasets in Apache Parquet format. It supports ML frameworks such as TensorFlow, Pytorch, and PySpark and can be used from pure Python code.\n\n**Required Libraries**: \n* `petastorm==0.8.2` via PyPI"],"metadata":{}},{"cell_type":"markdown","source":["Run the following cell to set up our environment."],"metadata":{}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["## Load data"],"metadata":{}},{"cell_type":"code","source":["from sklearn.datasets.california_housing import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\nimport pandas as pd\nnp.random.seed(0)\n\ncal_housing = fetch_california_housing()\n\n# split 80/20 train-test\nX_train, X_test, y_train, y_test = train_test_split(cal_housing.data,\n                                                        cal_housing.target,\n                                                        test_size=0.2,\n                                                        random_state=1)\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["## Spark DataFrame\n\nLet's concatenate our features and label, then create a Spark DataFrame from our Pandas DataFrame."],"metadata":{}},{"cell_type":"code","source":["data = pd.concat([pd.DataFrame(X_train, columns=cal_housing.feature_names), pd.DataFrame(y_train, columns=[\"label\"])], axis=1)\ntrainDF = spark.createDataFrame(data)\ndisplay(trainDF)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["## Create Dense Vectors for Features"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\n\nvecAssembler = VectorAssembler(inputCols=cal_housing.feature_names, outputCol=\"features\")\nvecTrainDF = vecAssembler.transform(trainDF).select(\"features\", \"label\")\ndisplay(vecTrainDF)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["## Array\n\nPetastorm requires an Array as input, not a Vector. Let's register a UDF in Scala and invoke it from Python for optimal performance."],"metadata":{}},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.linalg.Vector\nval toArray = udf { v: Vector => v.toArray }\nspark.udf.register(\"toArray\", toArray)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["## Save Data \n\nLet's write our DataFrame out as a parquet files to DBFS."],"metadata":{}},{"cell_type":"code","source":["file_path = f\"{workingDir}/deep-learning/petastorm.parquet\"\nvecTrainDF.selectExpr(\"toArray(features) AS features\", \"label\").repartition(8).write.mode(\"overwrite\").parquet(file_path)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["## Remove committed/started metadata\n\nPetastorm + Horovod do not work if you leave the committed/started metadata files in our Parquet folder. We will need to remove them."],"metadata":{}},{"cell_type":"code","source":["[dbutils.fs.rm(i.path) for i in dbutils.fs.ls(file_path) if (\"_committed_\" in i.name) | (\"_started_\" in i.name)]\n\ndisplay(dbutils.fs.ls(file_path))"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["## Define Model"],"metadata":{}},{"cell_type":"code","source":["import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import models, layers\ntf.set_random_seed(42)\n\ndef build_model():\n  from tensorflow.keras import models, layers\n  model = models.Sequential()\n  model.add(layers.Dense(20, input_dim=8, activation='relu'))\n  model.add(layers.Dense(20, activation='relu'))\n  model.add(layers.Dense(1, activation='linear'))\n  return model"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["## Single Node\n\nDefine shape of the input tensor and output tensor and fit the model (on the driver). We need to use Petastorm's [make_batch_reader](https://petastorm.readthedocs.io/en/latest/api.html#petastorm.reader.make_batch_reader) to create an instance of Reader for reading batches out of a non-Petastorm Parquet store."],"metadata":{}},{"cell_type":"code","source":["from petastorm import make_batch_reader\nfrom petastorm.tf_utils import make_petastorm_dataset\n\nabs_file_path = file_path.replace(\"dbfs:/\", \"/dbfs/\")\n\nwith make_batch_reader(\"file://\" + abs_file_path, num_epochs=None) as reader: \n  dataset = make_petastorm_dataset(reader).map(lambda x: (tf.reshape(x.features, [-1,8]), tf.reshape(x.label, [-1,1])))\n  model = build_model()\n  optimizer = keras.optimizers.Adam(lr=0.001)\n  model.compile(optimizer=optimizer,\n                loss='mse',\n                metrics=['mse'])\n  model.fit(dataset, steps_per_epoch=10, epochs=10)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["## Horovod\n\nLet's do the same thing, but let's add in Horovod for distributed model training."],"metadata":{}},{"cell_type":"code","source":["import horovod.tensorflow.keras as hvd\n\ndef run_training_horovod():\n  # Horovod: initialize Horovod.\n  hvd.init()\n  with make_batch_reader(\"file://\" + abs_file_path, num_epochs=None, cur_shard=hvd.rank(), shard_count= hvd.size()) as reader:\n    dataset = make_petastorm_dataset(reader).map(lambda x: (tf.reshape(x.features, [-1,8]), tf.reshape(x.label, [-1,1])))\n    model = build_model()\n    from tensorflow.keras import optimizers\n    optimizer = optimizers.Adam(lr=0.001*hvd.size())\n    optimizer = hvd.DistributedOptimizer(optimizer)\n    model.compile(optimizer=optimizer,\n                  loss='mse',\n                  metrics=['mse'])\n    history = model.fit(dataset, steps_per_epoch=10, epochs=10)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["## Train on driver"],"metadata":{}},{"cell_type":"code","source":["from sparkdl import HorovodRunner\nhr = HorovodRunner(np=-1)\nhr.run(run_training_horovod)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["## Better Horovod"],"metadata":{}},{"cell_type":"code","source":["import horovod.tensorflow.keras as hvd\n\n\ndbutils.fs.rm(f\"{ml_working_path}/petastorm_checkpoint_weights.ckpt\", True)\ndef run_training_horovod():\n  # Horovod: initialize Horovod.\n  hvd.init()\n  with make_batch_reader(\"file://\" + abs_file_path, num_epochs=None, cur_shard=hvd.rank(), shard_count=hvd.size()) as reader:\n    dataset = make_petastorm_dataset(reader).map(lambda x: (tf.reshape(x.features, [-1,8]), tf.reshape(x.label, [-1,1])))\n    model = build_model()\n    from tensorflow.keras import optimizers\n    optimizer = optimizers.Adam(lr=0.001*hvd.size())\n    optimizer = hvd.DistributedOptimizer(optimizer)\n    model.compile(optimizer=optimizer,\n                  loss='mse',\n                  metrics=['mse'])\n    checkpoint_dir = f\"{ml_working_path}/petastorm_checkpoint_weights.ckpt\"\n    callbacks = [\n    hvd.callbacks.BroadcastGlobalVariablesCallback(0),\n    hvd.callbacks.MetricAverageCallback(),\n    hvd.callbacks.LearningRateWarmupCallback(warmup_epochs=5, verbose=1),\n    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"loss\", patience=10, verbose=1)\n    ]\n\n    if hvd.rank() == 0:\n      callbacks.append(tf.keras.callbacks.ModelCheckpoint(checkpoint_dir, save_weights_only=True))\n  \n    history = model.fit(dataset, steps_per_epoch=10, epochs=10, callbacks=callbacks)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["import horovod.tensorflow.keras as hvd\nfrom sparkdl import HorovodRunner\nhr = HorovodRunner(np=-1)\nhr.run(run_training_horovod)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["## Run on all workers"],"metadata":{}},{"cell_type":"code","source":["from sparkdl import HorovodRunner\nhr = HorovodRunner(np=0)\nhr.run(run_training_horovod)"],"metadata":{},"outputs":[],"execution_count":28}],"metadata":{"name":"2. Horovod Petastorm","notebookId":3701447417106869},"nbformat":4,"nbformat_minor":0}