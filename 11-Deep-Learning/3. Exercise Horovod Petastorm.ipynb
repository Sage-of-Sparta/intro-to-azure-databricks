{"cells":[{"cell_type":"markdown","source":["## Exercise: Horovod with Petastorm for training a deep learning model\n\nIn this exercise we are going to build a model on the Boston housing dataset and distribute the deep learning training process using both HorovodRunner and Petastorm.\n\n**Required Libraries**: \n* `petastorm==0.8.2` via PyPI"],"metadata":{}},{"cell_type":"markdown","source":["Run the following cell to set up our environment."],"metadata":{}},{"cell_type":"code","source":["%run \"../Includes/Classroom-Setup\""],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["## 1. Load and process data\n\nWe again load the Boston housing data. However, as we saw in the demo, for Horovod we want to shard the data before passing into HorovodRunner. \n\nFor the `get_dataset` function below, load the data, split into 80/20 train-test, standardize the features and return train and test sets."],"metadata":{}},{"cell_type":"code","source":["from sklearn.datasets import load_boston\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\ndef get_dataset(rank=0, size=1):\n  scaler = StandardScaler()\n  \n  boston_housing = load_boston()\n\n  # split 80/20 train-test\n  X_train, X_test, y_train, y_test = train_test_split(boston_housing.data,\n                                                          boston_housing.target,\n                                                          test_size=0.2,\n                                                          random_state=1)\n  \n  scaler.fit(X_train)\n  X_train = scaler.transform(X_train[rank::size])\n  y_train = y_train[rank::size]\n  X_test = scaler.transform(X_test[rank::size])\n  y_test = y_test[rank::size]\n  \n  return (X_train, y_train), (X_test, y_test)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["##2. Build Model\n\nUsing the same model from earlier, let's define our model architecture"],"metadata":{}},{"cell_type":"code","source":["import numpy as np\nnp.random.seed(0)\nimport tensorflow as tf\ntf.set_random_seed(42) # For reproducibility\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\ndef build_model():\n  return Sequential([Dense(50, input_dim=13, activation='relu'),\n                    Dense(20, activation='relu'),\n                    Dense(1, activation='linear')])"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["## 3. Horovod\n\nIn order to distribute the training of our Keras model with Horovod, we must define our `run_training_horovod` training function"],"metadata":{}},{"cell_type":"code","source":["# ANSWER\nimport horovod.tensorflow.keras as hvd\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.callbacks import *\n\ndef run_training_horovod():\n  # Horovod: initialize Horovod.\n  hvd.init()\n  print(f\"Rank is: {hvd.rank()}\")\n  print(f\"Size is: {hvd.size()}\")\n  \n  (X_train, y_train), (X_test, y_test) = get_dataset(hvd.rank(), hvd.size())\n  \n  model = build_model()\n  \n  from tensorflow.keras import optimizers\n  optimizer = optimizers.Adam(lr=0.001*hvd.size())\n  optimizer = hvd.DistributedOptimizer(optimizer)\n  model.compile(optimizer=optimizer, loss=\"mse\", metrics=[\"mse\"])\n  checkpoint_dir = f\"{ml_working_path}/horovod_checkpoint_weights_lab.ckpt\"\n  \n  callbacks = [\n    # Horovod: broadcast initial variable states from rank 0 to all other processes.\n    # This is necessary to ensure consistent initialization of all workers when\n    # training is started with random weights or restored from a checkpoint.\n    hvd.callbacks.BroadcastGlobalVariablesCallback(0),\n\n    # Horovod: average metrics among workers at the end of every epoch.\n    # Note: This callback must be in the list before the ReduceLROnPlateau,\n    # TensorBoard or other metrics-based callbacks.\n    hvd.callbacks.MetricAverageCallback(),\n\n    # Horovod: using `lr = 1.0 * hvd.size()` from the very beginning leads to worse final\n    # accuracy. Scale the learning rate `lr = 1.0` ---> `lr = 1.0 * hvd.size()` during\n    # the first five epochs. See https://arxiv.org/abs/1706.02677 for details.\n    hvd.callbacks.LearningRateWarmupCallback(warmup_epochs=5, verbose=1),\n    \n    # Reduce the learning rate if training plateaus.\n    ReduceLROnPlateau(patience=10, verbose=1)\n    \n  ]\n  \n  # Horovod: save checkpoints only on worker 0 to prevent other workers from corrupting them.\n  if hvd.rank() == 0:\n    callbacks.append(ModelCheckpoint(checkpoint_dir, save_weights_only=True))\n  \n  history = model.fit(X_train, y_train, callbacks=callbacks, validation_split=.2, epochs=30, batch_size=16, verbose=2)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["Let's now run our model on all workers."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\nfrom sparkdl import HorovodRunner\n\nhr = HorovodRunner(np=0)\nhr.run(run_training_horovod)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["## 4. Horovod with Petastorm\n\nWe're now going to build a distributed deep learning model capable of handling data in Apache Parquet format. To do so, we can use Horovod along with Petastorm. \n\nFirst let's load the Boston housing data, and create a Spark DataFrame from the training data."],"metadata":{}},{"cell_type":"code","source":["import pandas as pd\n\nboston_housing = load_boston()\n\n# split 80/20 train-test\nX_train, X_test, y_train, y_test = train_test_split(boston_housing.data,\n                                                        boston_housing.target,\n                                                        test_size=0.2,\n                                                        random_state=1)\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# concatenate our features and label, then create a Spark DataFrame from our Pandas DataFrame.\ndata = pd.concat([pd.DataFrame(X_train, columns=boston_housing.feature_names), \n                  pd.DataFrame(y_train, columns=[\"label\"])], axis=1)\ntrainDF = spark.createDataFrame(data)\ndisplay(trainDF)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["### Create Vectors\n\nUse the VectorAssembler to combine all the features (not including the label) into a single column called `features`."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\nfrom pyspark.ml.feature import VectorAssembler\n\nvecAssembler = VectorAssembler(inputCols=boston_housing.feature_names, outputCol=\"features\")\nvecTrainDF = vecAssembler.transform(trainDF).select(\"features\", \"label\")\ndisplay(vecTrainDF)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["Let's now create a UDF to convert our Vector into an Array."],"metadata":{}},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.linalg.Vector\nval toArray = udf { v: Vector => v.toArray }\nspark.udf.register(\"toArray\", toArray)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["Save the DataFrame out as a parquet file to DBFS. \n\nLet's remember to remove the committed and started metadata files in the Parquet folder! Horovod with Petastorm will not work otherwise."],"metadata":{}},{"cell_type":"code","source":["file_path = f\"{workingDir}/petastorm.parquet\"\nvecTrainDF.selectExpr(\"toArray(features) AS features\", \"label\").repartition(8).write.mode(\"overwrite\").parquet(file_path)\n[dbutils.fs.rm(i.path) for i in dbutils.fs.ls(file_path) if (\"_committed_\" in i.name) | (\"_started_\" in i.name)]"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["Let's now define our `run_training_horovod` to format our data using Petastorm and distribute the training of our Keras model using Horovod."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\nfrom petastorm import make_batch_reader\nfrom petastorm.tf_utils import make_petastorm_dataset\nimport horovod.tensorflow.keras as hvd\n\nabs_file_path = file_path.replace(\"dbfs:/\", \"/dbfs/\")\n\ndef run_training_horovod():\n  # Horovod: initialize Horovod.\n  hvd.init()\n  with make_batch_reader(\"file://\" + abs_file_path, \n                         num_epochs=100, \n                         cur_shard=hvd.rank(), \n                         shard_count=hvd.size()) as reader:\n    \n    dataset = make_petastorm_dataset(reader).map(lambda x: (tf.reshape(x.features, [-1,13]),\n                                                            tf.reshape(x.label, [-1,1])))\n    \n    model = build_model()\n    from tensorflow.keras import optimizers\n    optimizer = optimizers.Adam(lr=0.001*hvd.size())\n    optimizer = hvd.DistributedOptimizer(optimizer)\n    model.compile(optimizer=optimizer, loss='mse')\n    \n    checkpoint_dir = f\"{ml_working_path}/petastorm_checkpoint_weights_lab.ckpt\"\n    \n    callbacks = [\n      hvd.callbacks.BroadcastGlobalVariablesCallback(0),\n      hvd.callbacks.MetricAverageCallback(),\n      hvd.callbacks.LearningRateWarmupCallback(warmup_epochs=5, verbose=1),\n      ReduceLROnPlateau(monitor=\"loss\", patience=10, verbose=1)\n    ]\n\n    # Horovod: save checkpoints only on worker 0 to prevent other workers from corrupting them.\n    if hvd.rank() == 0:\n      callbacks.append(ModelCheckpoint(checkpoint_dir, save_weights_only=True))\n\n    history = model.fit(dataset, callbacks=callbacks, steps_per_epoch=10, epochs=10)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["Finally, let's run our newly define Horovod training function with Petastorm to run across all workers."],"metadata":{}},{"cell_type":"code","source":["# ANSWER\nfrom sparkdl import HorovodRunner\n\nhr = HorovodRunner(np=0)\nhr.run(run_training_horovod)"],"metadata":{},"outputs":[],"execution_count":23}],"metadata":{"name":"3. Exercise Horovod Petastorm","notebookId":3701447417107137},"nbformat":4,"nbformat_minor":0}